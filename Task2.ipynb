{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from rdflib import Graph, Literal, BNode, Namespace, RDF, URIRef\n",
    "\n",
    "with open('graph.pkl', 'rb') as f:\n",
    "    g = pickle.load(f)\n",
    "\n",
    "\n",
    "# Define the ARXIV namespace\n",
    "ARXIV = Namespace(\"http://arxiv.org/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ragha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from rdflib import URIRef, Namespace, Graph\n",
    "from rdflib.namespace import RDF\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import lil_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained model and tokenizer (BERT)\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the device\n",
    "bert_model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the get_bert_embedding function to use bert_model instead of model\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = inputs.to(device)  # Move the inputs to the device\n",
    "    outputs = bert_model(**inputs)  # Adjust this line to use bert_model\n",
    "    # Use the [CLS] token representation as the sentence embedding\n",
    "    sentence_embedding = outputs.last_hidden_state[:, 0, :].detach().cpu().numpy()  # Move the tensor back to the CPU for numpy conversion\n",
    "    return sentence_embedding.squeeze()  # Remove extra dimensions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARXIV = Namespace(\"http://arxiv.org/\")\n",
    "\n",
    "papers = []\n",
    "paper_index = {}  # to quickly find the index of a paper by its id\n",
    "embeddings = []\n",
    "for s in g.subjects(RDF.type, ARXIV.Paper):\n",
    "    paper_id = str(s).replace(str(ARXIV), \"\")\n",
    "    paper_index[paper_id] = len(papers)\n",
    "    title = g.value(s, ARXIV.title)\n",
    "    abstract = g.value(s, ARXIV.abstract)\n",
    "    papers.append({\n",
    "        'paper_id': paper_id,\n",
    "        'title': str(title),\n",
    "        'abstract': {'text': str(abstract)}\n",
    "    })\n",
    "    embeddings.append(get_bert_embedding(str(abstract)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sparse matrix where each row represents a paper and each column represents a cited paper\n",
    "citation_matrix = lil_matrix((len(papers), len(papers)))\n",
    "\n",
    "for s, o in g.subject_objects(ARXIV.cites):\n",
    "    paper_id = str(s).replace(str(ARXIV), \"\")\n",
    "    cited_paper_id = str(o).replace(str(ARXIV), \"\")\n",
    "    if paper_id in paper_index and cited_paper_id in paper_index:  # only consider citations between papers that are in our graph\n",
    "        citation_matrix[paper_index[paper_id], paper_index[cited_paper_id]] = 1\n",
    "\n",
    "# Convert the citation matrix to CSR format for efficient arithmetic and matrix vector operations\n",
    "citation_matrix = citation_matrix.tocsr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(papers)\n",
    "pairs = np.zeros((n*n, 2), dtype=int)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        pairs[i*n + j, 0] = i\n",
    "        pairs[i*n + j, 1] = j\n",
    "\n",
    "# Calculate similarities\n",
    "similarities = cosine_similarity(np.array(embeddings))\n",
    "\n",
    "\n",
    "# Compute pair-wise similarities\n",
    "pair_similarities = similarities[pairs[:, 0], pairs[:, 1]].reshape(-1, 1)\n",
    "\n",
    "# Create target array\n",
    "y = citation_matrix.toarray().flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score: 0.866199533997673\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(pair_similarities, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model on the training set\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"ROC AUC Score: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(paper):\n",
    "    paper_vector = get_bert_embedding(paper['abstract'])\n",
    "    pair_similarities = cosine_similarity(paper_vector.reshape(1, -1), embeddings)\n",
    "    # flatten it to make it compatible with model.predict_proba\n",
    "    pair_similarities = pair_similarities.flatten().reshape(-1, 1)\n",
    "    scores = model.predict_proba(pair_similarities)[:, 1]\n",
    "    recommendations = sorted(zip(papers, scores), key=lambda x: -x[1])\n",
    "    # Modify this line to return the abstract as well\n",
    "    return [(recommendation[0]['paper_id'], recommendation[0]['abstract']) for recommendation in recommendations[:5]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for 'Enhanced Accuracy in Galactic Disc Action Estimates through Perturbed Distribution Functions' (Physics):\n",
      "ID: 2012.06597\n",
      "Abstract: {'text': '  In the Gaia era, understanding the effects of the perturbations of the\\nGalactic disc is of major importance in the context of dynamical modelling. In\\nthis theoretical paper we extend previous work in which, making use of the\\nepicyclic approximation, the linearized Boltzmann equation had been used to\\nexplicitly compute, away from resonances, the perturbed distribution function\\nof a Galactic thin-disc population in the presence of a non-axisymmetric\\nperturbation of constant amplitude. Here we improve this theoretical framework\\nin two distinct ways in the new code that we present. First, we use better\\nestimates for the action-angle variables away from quasi-circular orbits,\\ncomputed from the AGAMA software, and we present an efficient routine to\\nnumerically re-express any perturbing potential in these coordinates with a\\ntypical accuracy at the per cent level. The use of more accurate action\\nestimates allows us to identify resonances such as the outer 1:1 bar resonance\\nat higher azimuthal velocities than the outer Lindblad resonance (OLR), and to\\nextend our previous theoretical results well above the Galactic plane, where we\\nexplicitly show how they differ from the epicyclic approximation. In\\nparticular, the displacement of resonances in velocity space as a function of\\nheight can in principle constrain the 3D structure of the Galactic potential.\\nSecond, we allow the perturbation to be time dependent, thereby allowing us to\\nmodel the effect of transient spiral arms or a growing bar. The theoretical\\nframework and tools presented here will be useful for a thorough analytical\\ndynamical modelling of the complex velocity distribution of disc stars as\\nmeasured by past and upcoming Gaia data releases.\\n'}\n",
      "\n",
      "ID: 2008.08404\n",
      "Abstract: {'text': '  The eROSITA X-ray telescope on board the Spectrum-Roentgen-Gamma (SRG)\\nmission will measure the position and properties of about 100,000 clusters of\\ngalaxies and 3 million active galactic nuclei over the full sky. To study the\\nstatistical properties of this ongoing survey, it is key to estimate the\\nselection function accurately. We create a set of full sky light-cones using\\nthe MultiDark and UNIT dark matter only N-body simulations. We present a novel\\nmethod to predict the X-ray emission of galaxy clusters. Given a set of dark\\nmatter halo properties (mass, redshift, ellipticity, offset parameter), we\\nconstruct an X-ray emissivity profile and image for each halo in the\\nlight-cone. We follow the eROSITA scanning strategy to produce a list of X-ray\\nphotons on the full sky. We predict scaling relations for the model clusters,\\nwhich are in good agreement with the literature. The predicted number density\\nof clusters as a function of flux also agrees with previous measurements.\\nFinally, we obtain a scatter of 0.21 (0.07, 0.25) for the X-ray luminosity --\\nmass (temperature -- mass, luminosity -- temperature) model scaling relations.\\nWe provide catalogues with the model photons emitted by clusters and active\\ngalactic nuclei. These catalogues will aid the eROSITA end to end simulation\\nflow analysis and in particular the source detection process and cataloguing\\nmethods.\\n'}\n",
      "\n",
      "ID: 2012.08491\n",
      "Abstract: {'text': '  We used dedicated SRG/eROSITA X-ray, ASKAP/EMU radio, and DECam optical\\nobservations of a 15 sq.deg region around the interacting galaxy cluster system\\nA3391/95 to study the warm-hot gas in cluster outskirts and filaments, the\\nsurrounding large-scale structure and its formation process. We relate the\\nobservations to expectations from cosmological hydrodynamic simulations from\\nthe Magneticum suite.\\n  We trace the irregular morphology of warm-hot gas of the main clusters from\\ntheir centers out to well beyond their characteristic radii, $r_{200}$. Between\\nthe two main cluster systems, we observe an emission bridge; thanks to\\neROSITA\\'s unique soft response and large field of view, we discover tantalizing\\nhints for warm gas. Several matter clumps physically surrounding the system are\\ndetected. For the \"Northern Clump,\" we provide evidence that it is falling\\ntowards A3391 from the hot gas morphology and radio lobe structure of its\\ncentral AGN. Many of the extended sources in the field detected by eROSITA are\\nknown clusters or new clusters in the background, including a known SZ cluster\\nat redshift z=1. We discover an emission filament north of the virial radius,\\n$r_{100}$, of A3391 connecting to the Northern Clump and extending south of\\nA3395 towards another galaxy cluster. The total projected length of this\\ncontinuous warm-hot emission filament is 15 Mpc, running almost 4 degrees\\nacross the entire eROSITA observation. The DECam galaxy density map shows\\ngalaxy overdensities in the same regions. The new datasets provide impressive\\nconfirmation of the theoretically expected structure formation processes on the\\nindividual system level, including the surrounding warm-hot intergalactic\\nmedium distribution compared to the Magneticum simulation. Our spatially\\nresolved findings show that baryons indeed reside in large-scale warm-hot gas\\nfilaments with a clumpy structure.\\n'}\n",
      "\n",
      "ID: 2009.00327\n",
      "Abstract: {'text': '  We present a major update to the 3D coronal rope ejection (3DCORE) technique\\nfor modeling coronal mass ejection flux ropes in conjunction with an\\nApproximate Bayesian Computation (ABC) algorithm that is used for fitting the\\nmodel to in situ magnetic field measurements. The model assumes an empirically\\nmotivated torus-like flux rope structure that expands self-similarly within the\\nheliosphere, is influenced by a simplified interaction with the solar wind\\nenvironment, and carries along an embedded analytical magnetic field. The\\nimproved 3DCORE implementation allows us to generate extremely large ensemble\\nsimulations which we then use to find global best-fit model parameters using an\\nABC sequential Monte Carlo (SMC) algorithm. The usage of this algorithm, under\\nsome basic assumptions on the uncertainty of the magnetic field measurements,\\nallows us to furthermore generate estimates on the uncertainty of model\\nparameters using only a single in situ observation. We apply our model to\\nsynthetically generated measurements to prove the validity of our\\nimplementation for the fitting procedure. We also present a brief analysis,\\nwithin the scope of our model, of an event captured by Parker Solar Probe (PSP)\\nshortly after its first fly-by of the Sun on 2018 November 12 at 0.25 AU. The\\npresented toolset is also easily extendable to the analysis of events captured\\nby multiple spacecraft and will therefore facilitate future multi-point\\nstudies.\\n'}\n",
      "\n",
      "ID: 2012.12284\n",
      "Abstract: {'text': '  We investigate the morphology of the stellar distribution in a sample of\\nMilky Way (MW) like galaxies in the TNG50 simulation. Using a local in shell\\niterative method (LSIM) as the main approach, we explicitly show evidence of\\ntwisting (in about 52% of halos) and stretching (in 48% of them) in the real\\nspace. This is matched with the re-orientation observed in the eigenvectors of\\nthe inertia tensor and gives us a clear picture of having a re-oriented stellar\\ndistribution. We make a comparison between the shape profile of dark matter\\n(DM) halo and stellar distribution and quite remarkably see that their radial\\nprofiles are fairly close, especially at small galactocentric radii where the\\nstellar disk is located. This implies that the DM halo is somewhat aligned with\\nstars in response to the baryonic potential. The level of alignment mostly\\ndecreases away from the center. We study the impact of substructures in the\\norbital circularity parameter. It is demonstrated that in some cases, far away\\nsubstructures are counter-rotating compared with the central stars and may flip\\nthe sign of total angular momentum and thus the orbital circularity parameter.\\nTruncating them above 150 kpc, however, retains the disky structure of the\\ngalaxy as per initial selection. Including the impact of substructures in the\\nshape of stars, we explicitly show that their contribution is subdominant.\\nOverlaying our theoretical results to the observational constraints from\\nprevious literature, we establish fair agreement.\\n'}\n",
      "\n",
      "Recommendations for 'A multimodal analysis of Parkinson's disease patients' (Statistics):\n",
      "ID: 2002.05411\n",
      "Abstract: {'text': \"  Background and objectives: Parkinson's disease is a neurological disorder\\nthat affects the motor system producing lack of coordination, resting tremor,\\nand rigidity. Impairments in handwriting are among the main symptoms of the\\ndisease. Handwriting analysis can help in supporting the diagnosis and in\\nmonitoring the progress of the disease. This paper aims to evaluate the\\nimportance of different groups of features to model handwriting deficits that\\nappear due to Parkinson's disease; and how those features are able to\\ndiscriminate between Parkinson's disease patients and healthy subjects.\\n  Methods: Features based on kinematic, geometrical and non-linear dynamics\\nanalyses were evaluated to classify Parkinson's disease and healthy subjects.\\nClassifiers based on K-nearest neighbors, support vector machines, and random\\nforest were considered.\\n  Results: Accuracies of up to $93.1\\\\%$ were obtained in the classification of\\npatients and healthy control subjects. A relevance analysis of the features\\nindicated that those related to speed, acceleration, and pressure are the most\\ndiscriminant. The automatic classification of patients in different stages of\\nthe disease shows $\\\\kappa$ indexes between $0.36$ and $0.44$. Accuracies of up\\nto $83.3\\\\%$ were obtained in a different dataset used only for validation\\npurposes.\\n  Conclusions: The results confirmed the negative impact of aging in the\\nclassification process when we considered different groups of healthy subjects.\\nIn addition, the results reported with the separate validation set comprise a\\nstep towards the development of automated tools to support the diagnosis\\nprocess in clinical practice.\\n\"}\n",
      "\n",
      "ID: 2009.04518\n",
      "Abstract: {'text': \"  In living systems, we often see the emergence of the ingredients necessary\\nfor computation -- the capacity for information transmission, storage, and\\nmodification -- begging the question of how we may exploit or imitate such\\nbiological systems in unconventional computing applications. What can we gain\\nfrom artificial life in the advancement of computing technology? Artificial\\nlife provides us with powerful tools for understanding the dynamic behavior of\\nbiological systems and capturing this behavior in manmade substrates. With this\\napproach, we can move towards a new computing paradigm concerned with\\nharnessing emergent computation in physical substrates not governed by the\\nconstraints of Moore's law and ultimately realize massively parallel and\\ndistributed computing technology. In this paper, we argue that the lens of\\nartificial life offers valuable perspectives for the advancement of\\nhigh-performance computing technology. We first present a brief foundational\\nbackground on artificial life and some relevant tools that may be applicable to\\nunconventional computing. Two specific substrates are then discussed in detail:\\nbiological neurons and ensembles of nanomagnets. These substrates are the focus\\nof the authors' ongoing work, and they are illustrative of the two sides of the\\napproach outlined here -- the close study of living systems and the\\nconstruction of artificial systems to produce life-like behaviors. We conclude\\nwith a philosophical discussion on what we can learn from approaching\\ncomputation with the curiosity inherent to the study of artificial life. The\\nmain contribution of this paper is to present the great potential of using\\nartificial life methodologies to uncover and harness the inherent computational\\npower of physical substrates toward applications in unconventional\\nhigh-performance computing.\\n\"}\n",
      "\n",
      "ID: 1812.03503\n",
      "Abstract: {'text': '  We present an effective post-processing method to reduce the artifacts from\\nsparsely reconstructed cone-beam CT (CBCT) images. The proposed method is based\\non the state-of-the-art, image-to-image generative models with a perceptual\\nloss as regulation. Unlike the traditional CT artifact-reduction approaches,\\nour method is trained in an adversarial fashion that yields more perceptually\\nrealistic outputs while preserving the anatomical structures. To address the\\nstreak artifacts that are inherently local and appear across various scales, we\\nfurther propose a novel discriminator architecture based on feature pyramid\\nnetworks and a differentially modulated focus map to induce the adversarial\\ntraining. Our experimental results show that the proposed method can greatly\\ncorrect the cone-beam artifacts from clinical CBCT images reconstructed using\\n1/3 projections, and outperforms strong baseline methods both quantitatively\\nand qualitatively.\\n'}\n",
      "\n",
      "ID: 2001.08614\n",
      "Abstract: {'text': \"  Wikipedia, the free online encyclopedia that anyone can edit, is one of the\\nmost visited sites on the Web and a common source of information for many\\nusers. As an encyclopedia, Wikipedia is not a source of original information,\\nbut was conceived as a gateway to secondary sources: according to Wikipedia's\\nguidelines, facts must be backed up by reliable sources that reflect the full\\nspectrum of views on the topic. Although citations lie at the very heart of\\nWikipedia, little is known about how users interact with them. To close this\\ngap, we built client-side instrumentation for logging all interactions with\\nlinks leading from English Wikipedia articles to cited references during one\\nmonth, and conducted the first analysis of readers' interaction with citations\\non Wikipedia. We find that overall engagement with citations is low: about one\\nin 300 page views results in a reference click (0.29% overall; 0.56% on\\ndesktop; 0.13% on mobile). Matched observational studies of the factors\\nassociated with reference clicking reveal that clicks occur more frequently on\\nshorter pages and on pages of lower quality, suggesting that references are\\nconsulted more commonly when Wikipedia itself does not contain the information\\nsought by the user. Moreover, we observe that recent content, open access\\nsources and references about life events (births, deaths, marriages, etc) are\\nparticularly popular. Taken together, our findings open the door to a deeper\\nunderstanding of Wikipedia's role in a global information economy where\\nreliability is ever less certain, and source attribution ever more vital.\\n\"}\n",
      "\n",
      "ID: 2002.05412\n",
      "Abstract: {'text': \"  Parkinson's disease is a neurodegenerative disorder characterized by the\\npresence of different motor impairments. Information from speech, handwriting,\\nand gait signals have been considered to evaluate the neurological state of the\\npatients. On the other hand, user models based on Gaussian mixture models -\\nuniversal background models (GMM-UBM) and i-vectors are considered the\\nstate-of-the-art in biometric applications like speaker verification because\\nthey are able to model specific speaker traits. This study introduces the use\\nof GMM-UBM and i-vectors to evaluate the neurological state of Parkinson's\\npatients using information from speech, handwriting, and gait. The results show\\nthe importance of different feature sets from each type of signal in the\\nassessment of the neurological state of the patients.\\n\"}\n",
      "\n",
      "Recommendations for 'LOGO2-BongradPlus' (Computer Science):\n",
      "ID: 2010.00763\n",
      "Abstract: {'text': \"  Humans have an inherent ability to learn novel concepts from only a few\\nsamples and generalize these concepts to different situations. Even though\\ntoday's machine learning models excel with a plethora of training data on\\nstandard recognition tasks, a considerable gap exists between machine-level\\npattern recognition and human-level concept learning. To narrow this gap, the\\nBongard problems (BPs) were introduced as an inspirational challenge for visual\\ncognition in intelligent systems. Despite new advances in representation\\nlearning and learning to learn, BPs remain a daunting challenge for modern AI.\\nInspired by the original one hundred BPs, we propose a new benchmark\\nBongard-LOGO for human-level concept learning and reasoning. We develop a\\nprogram-guided generation technique to produce a large set of\\nhuman-interpretable visual cognition problems in action-oriented LOGO language.\\nOur benchmark captures three core properties of human cognition: 1)\\ncontext-dependent perception, in which the same object may have disparate\\ninterpretations given different contexts; 2) analogy-making perception, in\\nwhich some meaningful concepts are traded off for other meaningful concepts;\\nand 3) perception with a few samples but infinite vocabulary. In experiments,\\nwe show that the state-of-the-art deep learning methods perform substantially\\nworse than human subjects, implying that they fail to capture core human\\ncognition properties. Finally, we discuss research directions towards a general\\narchitecture for visual reasoning to tackle this benchmark.\\n\"}\n",
      "\n",
      "ID: 2011.02157\n",
      "Abstract: {'text': '  At the dawn of a new decade, particle physics faces the challenge of\\nexplaining the mystery of dark matter, the origin of matter over antimatter in\\nthe Universe, the apparent fine-tuning of the electro-weak scale, and many\\nother aspects of fundamental physics. Perhaps the most striking frontier to\\nemerge in the search for answers involves new physics at mass scales comparable\\nto familiar matter, below the GeV scale, but with very feeble interaction\\nstrength. New theoretical ideas to address dark matter and other fundamental\\nquestions predict such feebly interacting particles (FIPs) at these scales, and\\nindeed, existing data may even provide hints of this possibility. Emboldened by\\nthe lessons of the LHC, a vibrant experimental program to discover such physics\\nis under way, guided by a systematic theoretical approach firmly grounded on\\nthe underlying principles of the Standard Model. We give an overview of these\\nefforts, their motivations, and the decadal goals that animate the community\\ninvolved in the search for FIPs, with special focus on accelerator-based\\nexperiments.\\n'}\n",
      "\n",
      "ID: 2009.04518\n",
      "Abstract: {'text': \"  In living systems, we often see the emergence of the ingredients necessary\\nfor computation -- the capacity for information transmission, storage, and\\nmodification -- begging the question of how we may exploit or imitate such\\nbiological systems in unconventional computing applications. What can we gain\\nfrom artificial life in the advancement of computing technology? Artificial\\nlife provides us with powerful tools for understanding the dynamic behavior of\\nbiological systems and capturing this behavior in manmade substrates. With this\\napproach, we can move towards a new computing paradigm concerned with\\nharnessing emergent computation in physical substrates not governed by the\\nconstraints of Moore's law and ultimately realize massively parallel and\\ndistributed computing technology. In this paper, we argue that the lens of\\nartificial life offers valuable perspectives for the advancement of\\nhigh-performance computing technology. We first present a brief foundational\\nbackground on artificial life and some relevant tools that may be applicable to\\nunconventional computing. Two specific substrates are then discussed in detail:\\nbiological neurons and ensembles of nanomagnets. These substrates are the focus\\nof the authors' ongoing work, and they are illustrative of the two sides of the\\napproach outlined here -- the close study of living systems and the\\nconstruction of artificial systems to produce life-like behaviors. We conclude\\nwith a philosophical discussion on what we can learn from approaching\\ncomputation with the curiosity inherent to the study of artificial life. The\\nmain contribution of this paper is to present the great potential of using\\nartificial life methodologies to uncover and harness the inherent computational\\npower of physical substrates toward applications in unconventional\\nhigh-performance computing.\\n\"}\n",
      "\n",
      "ID: 2010.10783\n",
      "Abstract: {'text': '  Representation learning on user-item graph for recommendation has evolved\\nfrom using single ID or interaction history to exploiting higher-order\\nneighbors. This leads to the success of graph convolution networks (GCNs) for\\nrecommendation such as PinSage and LightGCN. Despite effectiveness, we argue\\nthat they suffer from two limitations: (1) high-degree nodes exert larger\\nimpact on the representation learning, deteriorating the recommendations of\\nlow-degree (long-tail) items; and (2) representations are vulnerable to noisy\\ninteractions, as the neighborhood aggregation scheme further enlarges the\\nimpact of observed edges.\\n  In this work, we explore self-supervised learning on user-item graph, so as\\nto improve the accuracy and robustness of GCNs for recommendation. The idea is\\nto supplement the classical supervised task of recommendation with an auxiliary\\nself-supervised task, which reinforces node representation learning via\\nself-discrimination. Specifically, we generate multiple views of a node,\\nmaximizing the agreement between different views of the same node compared to\\nthat of other nodes. We devise three operators to generate the views -- node\\ndropout, edge dropout, and random walk -- that change the graph structure in\\ndifferent manners. We term this new learning paradigm as\\n\\\\textit{Self-supervised Graph Learning} (SGL), implementing it on the\\nstate-of-the-art model LightGCN. Through theoretical analyses, we find that SGL\\nhas the ability of automatically mining hard negatives. Empirical studies on\\nthree benchmark datasets demonstrate the effectiveness of SGL, which improves\\nthe recommendation accuracy, especially on long-tail items, and the robustness\\nagainst interaction noises. Our implementations are available at\\n\\\\url{https://github.com/wujcan/SGL}.\\n'}\n",
      "\n",
      "ID: 2010.00403\n",
      "Abstract: {'text': '  The field of Artificial Intelligence (AI) is going through a period of great\\nexpectations, introducing a certain level of anxiety in research, business and\\nalso policy. This anxiety is further energised by an AI race narrative that\\nmakes people believe they might be missing out. Whether real or not, a belief\\nin this narrative may be detrimental as some stake-holders will feel obliged to\\ncut corners on safety precautions, or ignore societal consequences just to\\n\"win\". Starting from a baseline model that describes a broad class of\\ntechnology races where winners draw a significant benefit compared to others\\n(such as AI advances, patent race, pharmaceutical technologies), we investigate\\nhere how positive (rewards) and negative (punishments) incentives may\\nbeneficially influence the outcomes. We uncover conditions in which punishment\\nis either capable of reducing the development speed of unsafe participants or\\nhas the capacity to reduce innovation through over-regulation. Alternatively,\\nwe show that, in several scenarios, rewarding those that follow safety measures\\nmay increase the development speed while ensuring safe choices. Moreover, in\\n{the latter} regimes, rewards do not suffer from the issue of over-regulation\\nas is the case for punishment. Overall, our findings provide valuable insights\\ninto the nature and kinds of regulatory actions most suitable to improve safety\\ncompliance in the contexts of both smooth and sudden technological shifts.\\n'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_input_data(input_file):\n",
    "    input_papers = []\n",
    "    with open(input_file, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                input_papers.append(data)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping line as it's not valid JSON: {line.strip()}\")\n",
    "    return input_papers\n",
    "\n",
    "\n",
    "# Load the input data\n",
    "input_papers = load_input_data('new_research_papers.jsonl')\n",
    "\n",
    "# Generate recommendations for each input paper\n",
    "for paper in input_papers:\n",
    "    recommendations = recommend(paper)\n",
    "    print(f\"Recommendations for '{paper['title']}' ({paper['discipline']}):\")\n",
    "    for paper_id, abstract in recommendations:\n",
    "        print(f\"ID: {paper_id}\\nAbstract: {abstract}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
