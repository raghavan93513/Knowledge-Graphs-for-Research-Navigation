{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages and Loading the knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from rdflib import Literal, Namespace\n",
    "from rdflib.extras.external_graph_libs import rdflib_to_networkx_digraph\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "with open('graph.pkl', 'rb') as f:\n",
    "    g = pickle.load(f)\n",
    "\n",
    "\n",
    "# Define the ARXIV namespace\n",
    "ARXIV = Namespace(\"http://arxiv.org/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pagerank\n",
    "PageRank is an algorithm used by Google Search to rank websites in their search engine results, based on the concept that more important websites are likely to receive more links from other sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper ID: 1605.02688, PageRank Score: 0.0005306937790931206\n",
      "Paper ID: 1011.0352, PageRank Score: 0.0004946854805135191\n",
      "Paper ID: 1412.6980, PageRank Score: 0.0004745626324922857\n",
      "Paper ID: quant-ph/9705052, PageRank Score: 0.0004000020722544107\n",
      "Paper ID: 1105.4464, PageRank Score: 0.0003678372466247786\n"
     ]
    }
   ],
   "source": [
    "# Function to convert RDF graph to NetworkX graph\n",
    "def rdf_to_nx(g):\n",
    "    nx_graph = nx.MultiDiGraph()\n",
    "    for s, p, o in g:\n",
    "        if isinstance(o, Literal):\n",
    "            continue  # skip literals\n",
    "        if p == ARXIV.cites:\n",
    "            nx_graph.add_edge(s, o)\n",
    "    return nx_graph\n",
    "\n",
    "# Convert RDF graph to NetworkX graph\n",
    "nx_graph = rdf_to_nx(g)\n",
    "\n",
    "# Compute PageRank\n",
    "pagerank_scores = nx.pagerank(nx_graph)\n",
    "\n",
    "# Get the paper IDs with highest PageRank scores\n",
    "top_papers = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the paper IDs with highest PageRank scores\n",
    "for paper, score in top_papers[:5]:\n",
    "    paper = paper.replace(\"http://arxiv.org/\", \"\")\n",
    "    print(f'Paper ID: {paper}, PageRank Score: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HIT Score\n",
    "HITS (Hyperlink-Induced Topic Search) is a link analysis algorithm that assigns two scores for every page, 'authority', which estimates the value of the content of the page, and 'hub', which estimates the value of its links to other pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: 1207.7214, Authority Score: 0.0010448481862151613\n",
      "Node: 1207.7235, Authority Score: 0.001044848186215161\n",
      "Node: 1201.4330, Authority Score: 0.001020359768844416\n",
      "Node: 1910.06275, Authority Score: 0.000999650972012424\n",
      "Node: 1712.09737, Authority Score: 0.000999650972012424\n",
      "\n",
      "Node: 2009.00516, Hub Score: 0.8604648613325913\n",
      "Node: 2008.06494, Hub Score: 0.03426114842935145\n",
      "Node: 1805.00736, Hub Score: 0.01974284603624696\n",
      "Node: 2007.08542, Hub Score: 0.0185946434793888\n",
      "Node: 1802.09886, Hub Score: 0.018535168449002274\n"
     ]
    }
   ],
   "source": [
    "# Compute HITS scores\n",
    "hub_scores, authority_scores = nx.hits(nx_graph)\n",
    "\n",
    "# Get the nodes with the highest authority scores\n",
    "top_authorities = sorted(authority_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the nodes with the highest authority scores\n",
    "for node, score in top_authorities[:5]:\n",
    "    node = node.replace(\"http://arxiv.org/\", \"\")\n",
    "    print(f'Node: {node}, Authority Score: {score}')\n",
    "\n",
    "print()\n",
    "\n",
    "top_hubs = sorted(hub_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "for node, score in top_hubs[:5]:\n",
    "    node = node.replace(\"http://arxiv.org/\", \"\")\n",
    "    print(f'Node: {node}, Hub Score: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvector Centrality Score\n",
    "Eigenvector Centrality Score is a measure used in network analysis that assigns relative scores to all nodes in the network based on the principle that connections to high-scoring nodes contribute more to the score of the node in question than equal connections to low-scoring nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper ID: 2009.00516, Eigenvector Centrality Score: 0.6946376053239093\n",
      "Paper ID: Paper, Eigenvector Centrality Score: 0.12768624963298328\n",
      "Paper ID: 2008.06494, Eigenvector Centrality Score: 0.05400320088322996\n",
      "Paper ID: 1805.00736, Eigenvector Centrality Score: 0.043124126025963494\n",
      "Paper ID: 2012.07714, Eigenvector Centrality Score: 0.03892029018082468\n"
     ]
    }
   ],
   "source": [
    "# Convert RDF graph to NetworkX graph\n",
    "G = rdflib_to_networkx_digraph(g)\n",
    "\n",
    "# Get all nodes which are papers (URIs that start with http://arxiv.org/)\n",
    "paper_nodes = [n for n in G.nodes() if str(n).startswith(\"http://arxiv.org/\")]\n",
    "\n",
    "# Create a subgraph of G that includes only paper nodes and the edges between them\n",
    "G_paper_subgraph = G.subgraph(paper_nodes)\n",
    "\n",
    "# Remove parallel edges, if any, by converting DiGraph to a simple Graph \n",
    "G_paper_simple = nx.Graph(G_paper_subgraph)\n",
    "\n",
    "# Calculate eigenvector centrality\n",
    "centrality = nx.eigenvector_centrality_numpy(G_paper_simple)\n",
    "\n",
    "# Print the top 5 papers by eigenvector centrality\n",
    "sorted_centrality = sorted(centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for paper_id, centrality_score in sorted_centrality[:5]:\n",
    "    paper_id = paper_id.replace(\"http://arxiv.org/\", \"\")\n",
    "    print(f'Paper ID: {paper_id}, Eigenvector Centrality Score: {centrality_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of  PageRank scores, Hub scores, and Eigenvector Centrality for each paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper ID: 2009.00516, Final Score: [0.66838086]\n",
      "Paper ID: 1605.02688, Final Score: [0.33351089]\n",
      "Paper ID: 1011.0352, Final Score: [0.29804897]\n",
      "Paper ID: 1412.6980, Final Score: [0.27853691]\n",
      "Paper ID: quant-ph/9705052, Final Score: [0.20516886]\n"
     ]
    }
   ],
   "source": [
    "# Normalizing the scores using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "pagerank_scores = {k: v for k, v in sorted(pagerank_scores.items(), key=lambda item: item[1])}\n",
    "hub_scores = {k: v for k, v in sorted(hub_scores.items(), key=lambda item: item[1])}\n",
    "centrality = {k: v for k, v in sorted(centrality.items(), key=lambda item: item[1])}\n",
    "\n",
    "scores = [pagerank_scores, hub_scores, centrality]\n",
    "normalized_scores = []\n",
    "\n",
    "for score in scores:\n",
    "    # Reshape the scores to fit the scaler\n",
    "    data = np.array(list(score.values())).reshape(-1, 1)\n",
    "    # Fit the scaler and transform the data\n",
    "    normalized = scaler.fit_transform(data)\n",
    "    # Map the normalized scores back to the paper ids\n",
    "    normalized_score = {k: v for k, v in zip(score.keys(), normalized)}\n",
    "    normalized_scores.append(normalized_score)\n",
    "\n",
    "# Calculate the final scores by averaging the normalized scores\n",
    "final_scores = {}\n",
    "weights = [1/3, 1/3, 1/3]  # weights for each score\n",
    "\n",
    "for paper_id in pagerank_scores.keys():\n",
    "    final_scores[paper_id] = sum(normalized_scores[i][paper_id]*weights[i] for i in range(3))\n",
    "\n",
    "# Sort the final scores and print the top 5 papers\n",
    "top_papers_final = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for paper, score in top_papers_final[:5]:\n",
    "    paper = paper.replace(\"http://arxiv.org/\", \"\")\n",
    "    print(f'Paper ID: {paper}, Final Score: {score}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
