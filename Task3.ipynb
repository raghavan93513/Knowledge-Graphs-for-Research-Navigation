{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import URIRef, Literal, Namespace\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "with open('graph.pkl', 'rb') as f:\n",
    "    g = pickle.load(f)\n",
    "\n",
    "\n",
    "# Define the ARXIV namespace\n",
    "ARXIV = Namespace(\"http://arxiv.org/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper ID: http://arxiv.org/1605.02688, PageRank Score: 0.0005306937790931206\n",
      "Paper ID: http://arxiv.org/1011.0352, PageRank Score: 0.0004946854805135191\n",
      "Paper ID: http://arxiv.org/1412.6980, PageRank Score: 0.0004745626324922857\n",
      "Paper ID: http://arxiv.org/quant-ph/9705052, PageRank Score: 0.0004000020722544107\n",
      "Paper ID: http://arxiv.org/1105.4464, PageRank Score: 0.0003678372466247786\n"
     ]
    }
   ],
   "source": [
    "# Function to convert RDF graph to NetworkX graph\n",
    "def rdf_to_nx(g):\n",
    "    nx_graph = nx.MultiDiGraph()\n",
    "    for s, p, o in g:\n",
    "        if isinstance(o, Literal):\n",
    "            continue  # skip literals\n",
    "        if p == ARXIV.cites:\n",
    "            nx_graph.add_edge(s, o)\n",
    "    return nx_graph\n",
    "\n",
    "# Convert RDF graph to NetworkX graph\n",
    "nx_graph = rdf_to_nx(g)\n",
    "\n",
    "# Compute PageRank\n",
    "pagerank_scores = nx.pagerank(nx_graph)\n",
    "\n",
    "# Get the paper IDs with highest PageRank scores\n",
    "top_papers = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the paper IDs with highest PageRank scores\n",
    "for paper, score in top_papers[:5]:\n",
    "    print(f'Paper ID: {paper}, PageRank Score: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HIT Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: http://arxiv.org/1207.7235, Authority Score: 0.001044848186215161\n",
      "Node: http://arxiv.org/1207.7214, Authority Score: 0.0010448481862151608\n",
      "Node: http://arxiv.org/1201.4330, Authority Score: 0.0010203597688444158\n",
      "Node: http://arxiv.org/1711.09572, Authority Score: 0.0009996509720124243\n",
      "Node: http://arxiv.org/1812.01491, Authority Score: 0.0009996509720124243\n",
      "\n",
      "Node: http://arxiv.org/2009.00516, Hub Score: 0.8604648613325917\n",
      "Node: http://arxiv.org/2008.06494, Hub Score: 0.03426114842935146\n",
      "Node: http://arxiv.org/1805.00736, Hub Score: 0.019742846036246896\n",
      "Node: http://arxiv.org/2007.08542, Hub Score: 0.018594643479388797\n",
      "Node: http://arxiv.org/1802.09886, Hub Score: 0.01853516844900223\n"
     ]
    }
   ],
   "source": [
    "# Compute HITS scores\n",
    "hub_scores, authority_scores = nx.hits(nx_graph)\n",
    "\n",
    "# Get the nodes with the highest authority scores\n",
    "top_authorities = sorted(authority_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the nodes with the highest authority scores\n",
    "for node, score in top_authorities[:5]:\n",
    "    print(f'Node: {node}, Authority Score: {score}')\n",
    "\n",
    "print()\n",
    "\n",
    "top_hubs = sorted(hub_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "for node, score in top_hubs[:5]:\n",
    "    print(f'Node: {node}, Hub Score: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper ID: http://arxiv.org/2009.00516, Eigenvector Centrality Score: 0.6946376053239086\n",
      "Paper ID: http://arxiv.org/Paper, Eigenvector Centrality Score: 0.12768624963298275\n",
      "Paper ID: http://arxiv.org/2008.06494, Eigenvector Centrality Score: 0.05400320088322981\n",
      "Paper ID: http://arxiv.org/1805.00736, Eigenvector Centrality Score: 0.043124126025963445\n",
      "Paper ID: http://arxiv.org/2012.07714, Eigenvector Centrality Score: 0.03892029018082457\n"
     ]
    }
   ],
   "source": [
    "from rdflib.extras.external_graph_libs import rdflib_to_networkx_digraph\n",
    "\n",
    "# Convert RDF graph to NetworkX graph\n",
    "G = rdflib_to_networkx_digraph(g)\n",
    "\n",
    "# Get all nodes which are papers (URIs that start with http://arxiv.org/)\n",
    "paper_nodes = [n for n in G.nodes() if str(n).startswith(\"http://arxiv.org/\")]\n",
    "\n",
    "# Create a subgraph of G that includes only paper nodes and the edges between them\n",
    "G_paper_subgraph = G.subgraph(paper_nodes)\n",
    "\n",
    "# Remove parallel edges, if any, by converting DiGraph to a simple Graph \n",
    "G_paper_simple = nx.Graph(G_paper_subgraph)\n",
    "\n",
    "# Calculate eigenvector centrality\n",
    "centrality = nx.eigenvector_centrality_numpy(G_paper_simple)\n",
    "\n",
    "# Print the top 5 papers by eigenvector centrality\n",
    "sorted_centrality = sorted(centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for paper_id, centrality_score in sorted_centrality[:5]:\n",
    "    print(f'Paper ID: {paper_id}, Eigenvector Centrality Score: {centrality_score}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
